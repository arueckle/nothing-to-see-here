# Adapter-Hub adapter entry
# Defines a single adapter entry in Adapter-Hub
# --------------------

# The name of the author(s) of this adapter.
author: "Jonas Pfeiffer"

# A bibtex citation of the work related to this adapter.
citation: |
  @article{pfeiffer20madx,
    title={{MAD-X}: An {A}dapter-based {F}ramework for {M}ulti-task {C}ross-lingual {T}ransfer},
    author={Pfeiffer, Jonas and Vuli\'{c}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
    journal={arXiv preprint},
    year={2020},
    url={https://arxiv.org/pdf/2005.00052.pdf},
  }

# The string identifier of the adapter architecture (must be available in architecture).
# Describes the adapter architecture used by this adapter
config: # TODO: REQUIRED
  # The name of the adapter config used by this adapter (a short name available in the `architectures` folder).
  # Example: pfeiffer
  using: houlsby
  # Overrides the default activation function of the specified adapter architecture.
  # Example: tanh
  non_linearity: gelu
  # Overrides the default reduction factor of the specified adapter architecture
  # Example: 64
  reduction_factor: 2

# The version to be downloaded if no version is explicitly stated.
default_version: "nd"

# A short description of this adapter.
description: |
  Adapter trained with the houlsby adapter architecture with a reduction factor of 2 and a gelu activation function.
  The invertible adapter is also trained with a reduction factor of 2 and a gelu activation function.
  The adapter was trained for 250k steps and a batch size of 64 on english wikipedia text.

# A contact email of the author(s).
email: "pfeiffer@ukp.informatik.tu-darmstadt"

# A list of different versions of this adapter available for download.
files:
  - sha1: "0d5a99ab276579464a896b0b23e7914d2bf5c317"
    sha256: "b3e08f4bca11edb5252408c777e37c36738b63e45a3379fd00cef8adb3c016f6"
    # Download URL pointing to a zip folder containing the adapter module.
    url: "https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cdo/bert-base-multilingual-cased/houlsby/cdo_houlsby_gelu_nd.zip"
    version: "nd"
  - sha1: "6c2da02c6e96cdc03e0e16623d63a5a61b8f682c"
    sha256: "7ae6a93d0cddcb3caa9230466aa9c3a20ae0939d3fe5d287f08b7d8dca0fd415"
    # Download URL pointing to a zip folder containing the adapter module.
    url: "https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cdo/bert-base-multilingual-cased/houlsby/cdo_houlsby_gelu.zip"
    version: "wd"
#   - ...

# A GitHub handle associated with the author(s).
github: "jopfeiff"

# The hidden size of the model
hidden_size: 768

# The string identifier of the pre-trained model (by which it is identified at Huggingface).
# Example: bert-base-uncased
model_name: "bert-base-multilingual-cased"

# The model type.
# Example: bert
model_type: bert

# The string identifier of the subtask this adapter belongs to.
subtask: wiki

# The string identifier of the task this adapter belongs to.
task: cdo

# A Twitter handle associated with the author(s).
twitter: "@PfeiffJo"

# The type of adapter (one of the options available in `adapter_type`.
type: "text_lang"

# A URL providing more information on this adapter/ the authors/ the organization.
url: "https://pfeiffer.ai"